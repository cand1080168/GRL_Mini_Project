{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPInxr1DCNWE"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUERN8TKOL7b"
      },
      "outputs": [],
      "source": [
        "!python -c \"import torch; print(torch.__version__)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIpZt6iCOU3Y"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Download the corresponding PyTorch Geometric module\n",
        "\"\"\"\n",
        "Assign to TORCH with what you get from the cell above. E.g., export TORCH=1.12.1+cu113\n",
        "\"\"\"\n",
        "%env TORCH=2.1.0+cu121\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS4JID2AP0YH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.loader import DataLoader, NeighborLoader\n",
        "from torch_geometric.data.data import Data\n",
        "import torch_geometric.utils as U\n",
        "from torch_geometric.datasets import TUDataset\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import MessagePassing, GraphConv, GCNConv, GINConv, global_mean_pool\n",
        "from torch_geometric.utils.convert import from_scipy_sparse_matrix, from_networkx, to_networkx\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "from scipy.stats.stats import pearsonr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3nPq4OfCnLr"
      },
      "source": [
        "# Data utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load or construct data"
      ],
      "metadata": {
        "id": "NGfjc9FgAG9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synth_data(max_size=9):\n",
        "    \"\"\"\n",
        "    Returns lists of adjacency matrices for a dataset consisting of various\n",
        "    synthetic graphs.\n",
        "    \"\"\"\n",
        "    A = []\n",
        "\n",
        "    for matrix_size in range(3, max_size+1):\n",
        "      #cycle graphs\n",
        "      cycle_graph = torch.zeros((matrix_size, matrix_size))\n",
        "\n",
        "      for i in range(matrix_size - 1):\n",
        "          cycle_graph[i, i+1] = 1\n",
        "      A.append(cycle_graph + cycle_graph.T)\n",
        "\n",
        "      #grid graphs\n",
        "\n",
        "      #complete graphs\n",
        "      A.append(torch.ones((matrix_size, matrix_size)))\n",
        "\n",
        "      #star graphs\n",
        "      star_graph = torch.zeros((matrix_size, matrix_size), dtype=int)\n",
        "      star_graph[0, 1:] = 1\n",
        "      star_graph[1:, 0] = 1\n",
        "      A.append(star_graph)\n",
        "\n",
        "      #E-R graphs\n",
        "      for _ in range(8):\n",
        "        edge_probability = 0.5\n",
        "        e_r_graph = torch.zeros((matrix_size, matrix_size), dtype=int)\n",
        "\n",
        "        for i in range(matrix_size):\n",
        "            for j in range(i + 1, matrix_size):\n",
        "                if np.random.rand() < edge_probability:\n",
        "                    e_r_graph[i, j] = 1\n",
        "                    e_r_graph[j, i] = 1\n",
        "        A.append(e_r_graph)\n",
        "\n",
        "        #B-A graphs\n",
        "        m = 2\n",
        "        ba_graph = nx.barabasi_albert_graph(matrix_size, m)\n",
        "        ba_graph_adj = torch.from_numpy(nx.to_numpy_array(ba_graph))\n",
        "        A.append(ba_graph_adj)\n",
        "\n",
        "        #W-S graphs\n",
        "        k = 2\n",
        "        p = 0.2\n",
        "        ws_graph = nx.watts_strogatz_graph(matrix_size, k, p)\n",
        "        ws_graph_adj = torch.from_numpy(nx.to_numpy_array(ws_graph))\n",
        "        A.append(ws_graph_adj)\n",
        "    return A"
      ],
      "metadata": {
        "id": "qG1gP6AWANgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(dataset_name='MUTAG', node_features_type='original'):\n",
        "    \"\"\"\n",
        "    Returns lists of adjacency matrices for a dataset in TUDataset.\n",
        "    \"\"\"\n",
        "    raw_dataset = TUDataset(root='data/TUDataset', name=dataset_name)\n",
        "    A = []\n",
        "    X = []\n",
        "    Y = []\n",
        "    for graph in raw_dataset:\n",
        "        adj_matrix = U.to_dense_adj(graph.edge_index).squeeze(0)\n",
        "        A.append(adj_matrix)\n",
        "        X.append(graph.x)\n",
        "        Y.append(graph.y)\n",
        "\n",
        "    if node_features_type == 'original':\n",
        "      pass\n",
        "    elif node_features_type == 'constant':\n",
        "      X = [torch.ones(a.shape[0], 1) for a in A]\n",
        "    elif node_features_type == 'random':\n",
        "      X = [torch.rand(a.shape[0], 1) for a in A]\n",
        "    return A"
      ],
      "metadata": {
        "id": "we2S-up2AC_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data"
      ],
      "metadata": {
        "id": "wkE1HzAsAD3D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7oWKeDUCrF6"
      },
      "outputs": [],
      "source": [
        "def preprocess_graphs_ged(adj_matrix_list):\n",
        "    n = len(adj_matrix_list)\n",
        "\n",
        "    num_samples = n // 2\n",
        "    for i in range(num_samples):\n",
        "      sample = {}\n",
        "      start = 2 * i\n",
        "      end = min(2 * i + 1, n)\n",
        "\n",
        "      sample['adj1'] = adj_matrix_list[start]\n",
        "      sample['adj2'] = adj_matrix_list[end]\n",
        "\n",
        "      graph1 = nx.from_numpy_array(sample['adj1'].numpy())\n",
        "      graph2 = nx.from_numpy_array(sample['adj2'].numpy())\n",
        "\n",
        "      norm_ged = 2 * next(nx.optimize_graph_edit_distance(graph1, graph2)) / (sample['adj1'].shape[0] + sample['adj2'].shape[0])\n",
        "      #norm_ged = 2 * nx.graph_edit_distance(graph1, graph2) / (sample['adj1'].shape[0] + sample['adj2'].shape[0])\n",
        "      sample['target'] = torch.from_numpy(np.array(np.exp(-norm_ged))).float()\n",
        "\n",
        "      degrees1 = torch.tensor([graph1.degree(node) for node in graph1.nodes])\n",
        "      degrees2 = torch.tensor([graph2.degree(node) for node in graph2.nodes])\n",
        "      tiled_degrees1 = torch.tile(degrees1, (5, 1)).T\n",
        "      tiled_degrees2 = torch.tile(degrees2, (5, 1)).T\n",
        "\n",
        "      traingles1 = torch.tensor([float(trig) for trig in nx.triangles(graph1).values()])\n",
        "      traingles2 = torch.tensor([float(trig) for trig in nx.triangles(graph2).values()])\n",
        "      tiled_traingles1 = torch.tile(traingles1, (5, 1)).T\n",
        "      tiled_traingles2 = torch.tile(traingles2, (5, 1)).T\n",
        "\n",
        "      sample['feats1'] = torch.hstack((tiled_degrees1, tiled_traingles1))\n",
        "      sample['feats2'] = torch.hstack((tiled_degrees2, tiled_traingles2))\n",
        "\n",
        "\n",
        "      #sample['feats1'] = torch.ones((sample['adj1'].shape[0], 10))\n",
        "      #sample['feats2'] = torch.ones((sample['adj2'].shape[0], 10))\n",
        "      yield sample\n",
        "\n",
        "def preprocess_graphs_feats(adj_matrix_list):\n",
        "    n = len(adj_matrix_list)\n",
        "\n",
        "    num_samples = n // 2\n",
        "    for i in range(num_samples):\n",
        "      sample = {}\n",
        "      start = 2 * i\n",
        "      end = min(2 * i + 1, n)\n",
        "\n",
        "      sample['adj1'] = adj_matrix_list[start]\n",
        "      sample['adj2'] = adj_matrix_list[end]\n",
        "\n",
        "      graph1 = nx.from_numpy_array(sample['adj1'].numpy())\n",
        "      graph2 = nx.from_numpy_array(sample['adj2'].numpy())\n",
        "\n",
        "      trig_graph1 = 1 if max(nx.triangles(graph1).values()) >= 1 else 0\n",
        "      trig_graph2 = 1 if max(nx.triangles(graph2).values()) >= 1 else 0\n",
        "\n",
        "      avg_clust_coeff_graph1 = 1 if nx.average_clustering(graph1) >= 0.5 else 0\n",
        "      avg_clust_coeff_graph2 = 1 if nx.average_clustering(graph2) >= 0.5 else 0\n",
        "\n",
        "      girth_graph1 = 1 if nx.girth(graph1) <= 3 else 0\n",
        "      girth_graph2 = 1 if nx.girth(graph2) <= 3 else 0\n",
        "\n",
        "      diameter1 = 1 if max([max(j.values()) for (i,j) in nx.shortest_path_length(graph1)]) <= 4 else 0\n",
        "      diameter2 = 1 if max([max(j.values()) for (i,j) in nx.shortest_path_length(graph2)]) <= 4 else 0\n",
        "\n",
        "      even_nodes1 = 1 if sample['adj1'].shape[0] % 2 else 0\n",
        "      even_nodes2 = 1 if sample['adj2'].shape[0] % 2 else 0\n",
        "\n",
        "      eps = 1e-8\n",
        "      feats1 = np.array([1., trig_graph1, diameter1, avg_clust_coeff_graph1, girth_graph1, diameter1, even_nodes1])\n",
        "      feats2 = np.array([1., trig_graph2, diameter2, avg_clust_coeff_graph2, girth_graph2, diameter2, even_nodes2])\n",
        "      # sample['target'] = 1. - torch.from_numpy(np.array(np.inner(feats1, feats2) / (np.linalg.norm(feats1) * np.linalg.norm(feats2)))).float()\n",
        "      sample['target'] = 1. - nn.CosineSimilarity(dim=0, eps=eps)(torch.from_numpy(feats1).float(), torch.from_numpy(feats2).float())\n",
        "\n",
        "      degrees1 = torch.tensor([graph1.degree(node) for node in graph1.nodes])\n",
        "      degrees2 = torch.tensor([graph2.degree(node) for node in graph2.nodes])\n",
        "      tiled_degrees1 = torch.tile(degrees1, (5, 1)).T\n",
        "      tiled_degrees2 = torch.tile(degrees2, (5, 1)).T\n",
        "\n",
        "      traingles1 = torch.tensor([float(trig) for trig in nx.triangles(graph1).values()])\n",
        "      traingles2 = torch.tensor([float(trig) for trig in nx.triangles(graph2).values()])\n",
        "      tiled_traingles1 = torch.tile(traingles1, (5, 1)).T\n",
        "      tiled_traingles2 = torch.tile(traingles2, (5, 1)).T\n",
        "\n",
        "      sample['feats1'] = torch.hstack((tiled_degrees1, tiled_traingles1))\n",
        "      sample['feats2'] = torch.hstack((tiled_degrees2, tiled_traingles2))\n",
        "\n",
        "      #sample['feats1'] = torch.ones((sample['adj1'].shape[0], 10))\n",
        "      #sample['feats2'] = torch.ones((sample['adj2'].shape[0], 10))\n",
        "      yield sample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH8k75WsjL6N"
      },
      "source": [
        "# GNN Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baselines"
      ],
      "metadata": {
        "id": "lVE-6fLiZDRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConstantBaseline(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout=0.5,\n",
        "                 mode='inner'):\n",
        "        super(ConstantBaseline, self).__init__()\n",
        "\n",
        "        self.const = torch.nn.Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, data) -> torch.Tensor:\n",
        "        return self.const"
      ],
      "metadata": {
        "id": "BFZb_fO_wtmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomBaseline(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout=0.5,\n",
        "                 mode='inner'):\n",
        "        super(RandomBaseline, self).__init__()\n",
        "        self.const = torch.nn.Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, data) -> torch.Tensor:\n",
        "        return torch.rand(1) + self.const - self.const"
      ],
      "metadata": {
        "id": "_NDBWk5PxH_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GNNs"
      ],
      "metadata": {
        "id": "h8DTiLViZFtv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP41Xl_syHhl"
      },
      "outputs": [],
      "source": [
        "class TensorBoard(nn.Module):\n",
        "  def __init__(self,output_dim):\n",
        "        super(TensorBoard, self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.setup_weights()\n",
        "        self.init_parameters()\n",
        "\n",
        "  def setup_weights(self):\n",
        "        \"\"\"\n",
        "        Defining weights.\n",
        "        \"\"\"\n",
        "        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.output_dim,\n",
        "                                                             self.output_dim))\n",
        "\n",
        "        self.weight_matrix_block = torch.nn.Parameter(torch.Tensor(1,\n",
        "                                                                   2 * self.output_dim))\n",
        "        self.bias = torch.nn.Parameter(torch.Tensor(1, 1))\n",
        "\n",
        "  def init_parameters(self):\n",
        "        torch.nn.init.normal_(self.weight_matrix)\n",
        "        torch.nn.init.normal_(self.weight_matrix_block)\n",
        "        torch.nn.init.normal_(self.bias)\n",
        "\n",
        "  def forward(self, embedding_1, embedding_2):\n",
        "        scoring = torch.matmul(torch.t(embedding_1.view(self.output_dim, 1)), self.weight_matrix)\n",
        "        scoring = torch.matmul(scoring, embedding_2.view(self.output_dim, 1))\n",
        "        combined_representation = torch.cat((embedding_1, embedding_2)).view(2 * self.output_dim, 1)\n",
        "        block_scoring = torch.matmul(self.weight_matrix_block, combined_representation)\n",
        "        scores = torch.nn.functional.relu(scoring + block_scoring + self.bias)\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLoXOUI4zhZE"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout=0.5,\n",
        "                 mode='inner'):\n",
        "        super(GNN, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: input feaure dimension\n",
        "            hidden_dim: hidden feature dimension\n",
        "            output_dim: output dimensions\n",
        "            n_layers: number of layers\n",
        "            dropout: dropout_ratio\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.mode = mode\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.conv_layers = nn.ModuleList([GraphConv(input_dim, hidden_dim)])\n",
        "        for i in range(1,n_layers):\n",
        "            self.conv_layers.append(GraphConv(hidden_dim, hidden_dim))\n",
        "\n",
        "        if mode not in ['inner', 'tensor_board']:\n",
        "          raise NotImplementedError('Please use a valid mode type')\n",
        "\n",
        "        if mode == 'tensor_board':\n",
        "          self.tensor_board = TensorBoard(self.output_dim)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def conv_pass(self, x, adj):\n",
        "        edge_index = U.dense_to_sparse(adj)[0].to(self.device)\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.conv_layers[i](x, edge_index)\n",
        "\n",
        "            if i < self.n_layers - 1:\n",
        "                x = F.relu(x) #Remove ReLU for the last layer\n",
        "\n",
        "            x = F.dropout(x, p=self.dropout, training = self.training)\n",
        "\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, data) -> torch.Tensor:\n",
        "        adj1 = data[\"adj1\"]\n",
        "        adj2 = data[\"adj2\"]\n",
        "        feats1 = data[\"feats1\"]\n",
        "        feats2 = data[\"feats2\"]\n",
        "\n",
        "        latent_feats1 = self.conv_pass(feats1, adj1)\n",
        "        latent_feats2 = self.conv_pass(feats2, adj2)\n",
        "\n",
        "        pooled_feats1 = torch.mean(latent_feats1, dim=0)\n",
        "        pooled_feats2 = torch.mean(latent_feats2, dim=0)\n",
        "\n",
        "        if self.mode == 'inner':\n",
        "          inner = 1. - torch.inner(pooled_feats1, pooled_feats2) / (torch.norm(pooled_feats1) * torch.norm(pooled_feats2))\n",
        "          return inner\n",
        "        else:\n",
        "          scoring = self.tensor_board(pooled_feats1, pooled_feats2)\n",
        "          return scoring\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def init_params(tensor_size: tuple, mean: float = 0.0, std: float = 1.0\n",
        "                ) -> nn.Parameter:\n",
        "    weights = nn.Parameter(torch.FloatTensor(tensor_size[0], tensor_size[1]))\n",
        "    stdv = 1. / math.sqrt(weights.size(1))\n",
        "    weights.data.uniform_(-stdv, stdv)\n",
        "    return weights"
      ],
      "metadata": {
        "id": "ICPxbdzMz-3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNLayerGlobalReadout(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "      super(GNNLayerGlobalReadout, self).__init__()\n",
        "\n",
        "      # read in parameters\n",
        "      self.input_dim, self.output_dim = input_dim, output_dim\n",
        "\n",
        "      # initialise weights\n",
        "      self.weights_self = init_params(tensor_size=(self.input_dim,\n",
        "                                                   self.output_dim),\n",
        "                                      mean=0.0, std=1.0)\n",
        "      self.weights_neigh = init_params(tensor_size=(self.input_dim,\n",
        "                                                   self.output_dim),\n",
        "                                      mean=0.0, std=1.0)\n",
        "      self.weights_global = init_params(tensor_size=(self.input_dim,\n",
        "                                                   self.output_dim),\n",
        "                                      mean=0.0, std=1.0)\n",
        "\n",
        "\n",
        "    def forward(self, node_feats, adj_matrix):\n",
        "      self_term = torch.matmul(node_feats, self.weights_self)\n",
        "      neigh_term = torch.matmul(adj_matrix.float(), torch.matmul(node_feats,\n",
        "                                                 self.weights_neigh))\n",
        "      global_term = torch.matmul(torch.ones(adj_matrix.shape), torch.matmul(node_feats,\n",
        "                                                 self.weights_global))\n",
        "      return torch.add(torch.add(self_term, neigh_term), global_term)"
      ],
      "metadata": {
        "id": "LsWDxIZgzTp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2pyiQRoJB2B"
      },
      "outputs": [],
      "source": [
        "class GNNGlobalReadout(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout=0.5,\n",
        "                 mode='inner'):\n",
        "        super(GNNGlobalReadout, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: input feaure dimension\n",
        "            hidden_dim: hidden feature dimension\n",
        "            output_dim: output dimensions\n",
        "            n_layers: number of layers\n",
        "            dropout: dropout_ratio\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.mode = mode\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.conv_layers = nn.ModuleList([GNNLayerGlobalReadout(input_dim, hidden_dim)])\n",
        "        for i in range(1,n_layers):\n",
        "            self.conv_layers.append(GNNLayerGlobalReadout(hidden_dim, hidden_dim))\n",
        "\n",
        "        if mode not in ['inner', 'tensor_board']:\n",
        "          raise NotImplementedError('Please use a valid mode type')\n",
        "\n",
        "        if mode == 'tensor_board':\n",
        "          self.tensor_board = TensorBoard(self.output_dim)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def conv_pass(self, x, adj):\n",
        "        edge_index = U.dense_to_sparse(adj)[0].to(self.device)\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.conv_layers[i](x, adj)\n",
        "\n",
        "            if i < self.n_layers - 1:\n",
        "                x = F.relu(x) #Remove ReLU for the last layer\n",
        "\n",
        "            x = F.dropout(x, p=self.dropout, training = self.training)\n",
        "\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, data) -> torch.Tensor:\n",
        "        adj1 = data[\"adj1\"]\n",
        "        adj2 = data[\"adj2\"]\n",
        "        feats1 = data[\"feats1\"]\n",
        "        feats2 = data[\"feats2\"]\n",
        "\n",
        "        latent_feats1 = self.conv_pass(feats1, adj1)\n",
        "        latent_feats2 = self.conv_pass(feats2, adj2)\n",
        "\n",
        "        pooled_feats1 = torch.mean(latent_feats1, dim=0)\n",
        "        pooled_feats2 = torch.mean(latent_feats2, dim=0)\n",
        "\n",
        "        if self.mode == 'inner':\n",
        "          inner = 1. - torch.inner(pooled_feats1, pooled_feats2) / (torch.norm(pooled_feats1) * torch.norm(pooled_feats2))\n",
        "          return inner\n",
        "        else:\n",
        "          scoring = self.tensor_board(pooled_feats1, pooled_feats2)\n",
        "          return scoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rk_FEL6jAi9J"
      },
      "outputs": [],
      "source": [
        "class GIN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout=0.5,\n",
        "                 mode='inner'):\n",
        "        super(GIN, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: input feaure dimension\n",
        "            hidden_dim: hidden feature dimension\n",
        "            output_dim: output dimensions\n",
        "            n_layers: number of layers\n",
        "            dropout: dropout_ratio\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.mode = mode\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.conv_layers = nn.ModuleList([GINConv(\n",
        "            Sequential(Linear(self.input_dim, self.hidden_dim),\n",
        "                       BatchNorm1d(self.hidden_dim), ReLU(),\n",
        "                       Linear(self.hidden_dim, self.hidden_dim), ReLU()))])\n",
        "        for i in range(1, self.n_layers):\n",
        "          self.conv_layers.append(GINConv(\n",
        "                                  Sequential(Linear(self.hidden_dim, self.hidden_dim), BatchNorm1d(self.hidden_dim), ReLU(),\n",
        "                                            Linear(self.hidden_dim, self.hidden_dim), ReLU())))\n",
        "        if mode not in ['inner', 'tensor_board']:\n",
        "          raise NotImplementedError('Please use a valid mode type')\n",
        "\n",
        "        if mode == 'tensor_board':\n",
        "          self.tensor_board = TensorBoard(self.output_dim)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def conv_pass(self, x, adj):\n",
        "        edge_index = U.dense_to_sparse(adj)[0].to(self.device)\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.conv_layers[i](x, edge_index)\n",
        "\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, data) -> torch.Tensor:\n",
        "        adj1 = data[\"adj1\"]\n",
        "        adj2 = data[\"adj2\"]\n",
        "        feats1 = data[\"feats1\"]\n",
        "        feats2 = data[\"feats2\"]\n",
        "\n",
        "        latent_feats1 = self.conv_pass(feats1, adj1)\n",
        "        latent_feats2 = self.conv_pass(feats2, adj2)\n",
        "\n",
        "        pooled_feats1 = torch.mean(latent_feats1, dim=0)\n",
        "        pooled_feats2 = torch.mean(latent_feats2, dim=0)\n",
        "\n",
        "        if self.mode == 'inner':\n",
        "          inner = 1. - torch.inner(pooled_feats1, pooled_feats2) / (torch.norm(pooled_feats1) * torch.norm(pooled_feats2))\n",
        "          return inner\n",
        "        else:\n",
        "          scoring = self.tensor_board(pooled_feats1, pooled_feats2)\n",
        "          return scoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D9BqR1zIP55"
      },
      "source": [
        "# Training and evaluation setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw0HfJBDiDlW"
      },
      "outputs": [],
      "source": [
        "def train(data_iterator, model, optimizer, criterion, device):\n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "    targets = []\n",
        "    predictions = []\n",
        "    for data in data_iterator:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(data).to(device)\n",
        "\n",
        "        loss = criterion(pred, data['target'])\n",
        "        losses.append(loss.item())\n",
        "        predictions.append(pred.detach().numpy())\n",
        "        targets.append(data['target'].detach().numpy())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return np.mean(losses), np.corrcoef(np.array(predictions).flatten(), np.array(targets))[0, 1]\n",
        "\n",
        "def eval(data_iterator, model, device):\n",
        "    model.eval()\n",
        "\n",
        "    losses = []\n",
        "    targets = []\n",
        "    predictions = []\n",
        "\n",
        "    for data in data_iterator:\n",
        "\n",
        "        pred = model(data).to(device)\n",
        "\n",
        "        loss = torch.mean(torch.square(pred - data['target']))\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        predictions.append(pred.detach().numpy())\n",
        "        targets.append(data['target'].detach().numpy())\n",
        "\n",
        "    return np.mean(losses), np.corrcoef(np.array(predictions).flatten(), np.array(targets))[0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz-UwLe6re5k"
      },
      "outputs": [],
      "source": [
        "def train_model(A_train, A_test, params, verbose):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    model = params['model'](params['input_dim'],\n",
        "                            params['hidden_dim'],\n",
        "                            params['output_dim'],\n",
        "                            params['n_layers'],\n",
        "                            params['dropout'],\n",
        "                            params['mode']).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=params['lr'],\n",
        "                           weight_decay=params['weight_decay'])\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    early_stopper = EarlyStopper(params['max_patience'])\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    train_correlations = []\n",
        "    test_correlations = []\n",
        "\n",
        "    for epoch in tqdm(range(1, params['epochs']+1)):\n",
        "        if params['similarity_measure'] == 'ged':\n",
        "          train_data = preprocess_graphs_ged(A_train)\n",
        "          test_data = preprocess_graphs_ged(A_test)\n",
        "        elif params['similarity_measure'] == 'feats':\n",
        "          train_data = preprocess_graphs_feats(A_train)\n",
        "          test_data = preprocess_graphs_feats(A_test)\n",
        "        else:\n",
        "          raise NotImplementedError('Please use a valid Similarity Measure')\n",
        "        train_loss, train_correlation = train(train_data,\n",
        "                          model,\n",
        "                          optimizer,\n",
        "                          criterion,\n",
        "                          device)\n",
        "        test_loss, test_correlation = eval(test_data,\n",
        "                        model,\n",
        "                        device)\n",
        "\n",
        "        epoch_len = len(str(params['epochs']))\n",
        "\n",
        "        print_msg = (f'[{epoch:>{epoch_len}}/{params[\"epochs\"]:>{epoch_len}}] ' +\n",
        "                        f'loss: {train_loss:.5f} ' +\n",
        "                        f'test loss: {test_loss:.5f} ' +\n",
        "                        f'train corrlation: {train_correlation}'\n",
        "                        f'test corrlation: {test_correlation}'\n",
        "                        )\n",
        "        if verbose:\n",
        "            print(print_msg)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        train_correlations.append(train_correlation)\n",
        "        test_correlations.append(test_correlation)\n",
        "\n",
        "    print('Train loss: {}'.format(train_losses[-1]),\n",
        "          'Test loss: {}'.format(test_losses[-1]),\n",
        "          'Train correlation: {}'.format(train_correlations[-1]),\n",
        "          'Test correlation: {}'.format(test_correlations[-1]))\n",
        "    return train_losses[-1], test_losses[-1-1], train_correlations[-1], test_correlations[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhE__u1kKjuZ"
      },
      "outputs": [],
      "source": [
        "def cross_val(A, params, verbose=False):\n",
        "    \"\"\"\n",
        "    10-fold cross-validation\n",
        "    \"\"\"\n",
        "    group_size = len(A)//10+1\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    train_correlations = []\n",
        "    test_correlations = []\n",
        "    for i in range(0, len(A), group_size):\n",
        "        print('Run {}/10...'.format(i//group_size + 1))\n",
        "\n",
        "        A_test = A[i: i+group_size]\n",
        "        A_train = A[:i] + A[i+group_size:]\n",
        "\n",
        "        train_acc, test_acc, train_corr, test_corr = train_model(A_train,\n",
        "                                                                 A_test,\n",
        "                                                                 params,\n",
        "                                                                 verbose)\n",
        "        train_losses.append(train_acc)\n",
        "        test_losses.append(test_acc)\n",
        "        train_correlations.append(train_corr)\n",
        "        test_correlations.append(test_corr)\n",
        "\n",
        "    print('Train accuracy:', np.mean(train_losses), '+- ', np.std(train_losses))\n",
        "    print('Test accuracy:', np.mean(test_losses), '+- ', np.std(test_losses))\n",
        "    print('Train correlation:', np.mean(train_correlations), '+- ', np.std(train_correlations))\n",
        "    print('Test correlation:', np.mean(test_correlations), '+- ', np.std(test_correlations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWW1m7d3KDzr"
      },
      "source": [
        "# Experiments (Inner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WbRejYo6Y80"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YGq0LUDHc10"
      },
      "source": [
        "## Graph Edit Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOWjeE3sNO7P"
      },
      "outputs": [],
      "source": [
        "A = get_synth_data(max_size=9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaAZnZQh4Obl"
      },
      "source": [
        "### GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m75fkbk2j9pF"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'input_dim': 10,\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 10,\n",
        "    'n_layers': 4,\n",
        "    'epochs': 15,\n",
        "    'model': GNN,\n",
        "    'mode': 'inner',\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0,\n",
        "    'max_patience': 5,\n",
        "    'dropout': 0.3,\n",
        "    'batch_size': 5,\n",
        "    'similarity_measure': 'ged'\n",
        "}\n",
        "\n",
        "cross_val(A, params, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyNJvULc4SYU"
      },
      "source": [
        "### GIN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43LasVYEG70m"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'input_dim': 10,\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 10,\n",
        "    'n_layers': 4,\n",
        "    'epochs': 15,\n",
        "    'model': GIN,\n",
        "    'mode': 'inner',\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0,\n",
        "    'max_patience': 5,\n",
        "    'dropout': 0.3,\n",
        "    'batch_size': 5,\n",
        "    'similarity_measure': 'ged'\n",
        "}\n",
        "\n",
        "cross_val(A, params, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXg7oFYb1Np2"
      },
      "source": [
        "### GNN (GR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HByRmygX1NqG"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'input_dim': 10,\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 10,\n",
        "    'n_layers': 4,\n",
        "    'epochs': 15,\n",
        "    'model': GNNGlobalReadout,\n",
        "    'mode': 'inner',\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0,\n",
        "    'max_patience': 5,\n",
        "    'dropout': 0.3,\n",
        "    'batch_size': 5,\n",
        "    'similarity_measure': 'ged'\n",
        "}\n",
        "\n",
        "cross_val(A, params, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-nsvbCu1X5h"
      },
      "source": [
        "### Baselines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66_PK6451X5h"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'input_dim': 10,\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 10,\n",
        "    'n_layers': 4,\n",
        "    'epochs': 15,\n",
        "    'model': RandomBaseline,\n",
        "    'mode': 'inner',\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0,\n",
        "    'max_patience': 5,\n",
        "    'dropout': 0.3,\n",
        "    'batch_size': 5,\n",
        "    'similarity_measure': 'ged'\n",
        "}\n",
        "\n",
        "cross_val(A, params, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'input_dim': 10,\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 10,\n",
        "    'n_layers': 4,\n",
        "    'epochs': 15,\n",
        "    'model': ConstantBaseline,\n",
        "    'mode': 'inner',\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0,\n",
        "    'max_patience': 5,\n",
        "    'dropout': 0.3,\n",
        "    'batch_size': 5,\n",
        "    'similarity_measure': 'ged'\n",
        "}\n",
        "\n",
        "cross_val(A, params, verbose=False)"
      ],
      "metadata": {
        "id": "4DPkvKokE-bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tERSC7bPDvj"
      },
      "source": [
        "## Feature Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-8ENZhYPbDi"
      },
      "outputs": [],
      "source": [
        "A = get_synth_data(max_size=9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW7pUYaASOiR"
      },
      "source": [
        "### GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YToJtjdKPzKK"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'input_dim': 10,\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 10,\n",
        "    'n_layers': 4,\n",
        "    'epochs': 5,\n",
        "    'model': GNN,\n",
        "    'mode': 'inner',\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0,\n",
        "    'max_patience': 5,\n",
        "    'dropout': 0.3,\n",
        "    'batch_size': 5,\n",
        "    'similarity_measure': 'feats'\n",
        "}\n",
        "\n",
        "cross_val(A, params, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnTF5NL5SYGv"
      },
      "source": [
        "### GIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0u48-ffZPuR"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'input_dim': 10,\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 10,\n",
        "    'n_layers': 4,\n",
        "    'epochs': 5,\n",
        "    'model': GIN,\n",
        "    'mode': 'inner',\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0,\n",
        "    'max_patience': 5,\n",
        "    'dropout': 0.3,\n",
        "    'batch_size': 5,\n",
        "    'similarity_measure': 'feats'\n",
        "}\n",
        "\n",
        "cross_val(A, params, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cISdUwM51CpY"
      },
      "source": [
        "### GNN (GR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4phDTLt1Cpm"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'input_dim': 10,\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 10,\n",
        "    'n_layers': 4,\n",
        "    'epochs': 5,\n",
        "    'model': GNNGlobalReadout,\n",
        "    'mode': 'inner',\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0,\n",
        "    'max_patience': 5,\n",
        "    'dropout': 0.3,\n",
        "    'batch_size': 5,\n",
        "    'similarity_measure': 'feats'\n",
        "}\n",
        "\n",
        "cross_val(A, params, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYdhYFqe1Cpm"
      },
      "source": [
        "### Baselines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCa9jUh11Cpm"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'input_dim': 10,\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 10,\n",
        "    'n_layers': 4,\n",
        "    'epochs': 15,\n",
        "    'model': RandomBaseline,\n",
        "    'mode': 'inner',\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0,\n",
        "    'max_patience': 5,\n",
        "    'dropout': 0.3,\n",
        "    'batch_size': 5,\n",
        "    'similarity_measure': 'feats'\n",
        "}\n",
        "\n",
        "cross_val(A, params, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'input_dim': 10,\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 10,\n",
        "    'n_layers': 4,\n",
        "    'epochs': 15,\n",
        "    'model': ConstantBaseline,\n",
        "    'mode': 'inner',\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0,\n",
        "    'max_patience': 5,\n",
        "    'dropout': 0.3,\n",
        "    'batch_size': 5,\n",
        "    'similarity_measure': 'feats'\n",
        "}\n",
        "\n",
        "cross_val(A, params, verbose=False)"
      ],
      "metadata": {
        "id": "CLapcLkA1Cpm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "EPInxr1DCNWE",
        "l3nPq4OfCnLr",
        "bH8k75WsjL6N"
      ],
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}